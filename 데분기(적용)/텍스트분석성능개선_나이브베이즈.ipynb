{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45ae3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c29f20f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/jun/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Sample: you think that these people only exist in the movies , but trust me , they ' re as real as life . i once talked to a guy who thought the united states government was putting satellites into orbit which could fry an individual person ' s brain with microwaves . then i sat in a room full of people who believed that the government rigged state elections . i even listened to a man who swore that nicotine was an additive that cigarette companies put in their products for the specific goal of getting people addicted . these...\n",
      "Label: neg\n"
     ]
    }
   ],
   "source": [
    "# NLTK의 movie_reviews 데이터셋 다운로드\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# 영화 리뷰와 해당 라벨을 데이터셋으로 로드\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# 데이터를 셔플링하여 무작위로 섞음\n",
    "random.shuffle(documents)\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "sample_review, sample_label = documents[0]\n",
    "print(f\"Review Sample: {' '.join(sample_review[:100])}...\")\n",
    "print(f\"Label: {sample_label}\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9690bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 리뷰 텍스트와 라벨 분리하고, 불용어 제거 \n",
    "reviews = [' '.join([word for word in review if word.lower() not in stop_words])for review, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2eefb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라벨을 분리\n",
    "label =[category for review, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91107ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습에 필요한 리뷰랑, 라벨을 분리하여 진행\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "# 리뷰데이터 임베딩 변환\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "642e683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터셋 나누기\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y ,test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12176a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navie Bayes 분류기 학습\n",
    "model =MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#예측값은\n",
    "y_pred= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a5954a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuarcy :0.8200\n"
     ]
    }
   ],
   "source": [
    "## 모델 정확도를 예측\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(f\"Base Model Accuarcy :{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07adaf",
   "metadata": {},
   "source": [
    "### 전처리를 어떤 식으로 하면 좋을까?\n",
    "- 단어 정규화를 통해 성능이 좋아지지 않을까?\n",
    "- 어간, 표제어 추출해서 성능을 좀 더 올릴 수 있지 않을까?\n",
    "- countvectorizer 빈도에 대한 것을 기준을 다르게 해서 성능을 좀 더 올릴 수 있지 않을까?\n",
    "    - 빈도가 너무 낮은 값들은 제거하고, 어느정도 빈도를 기준을 추가하는 것\n",
    "- TF-IDF로 진행해서 새롭게 벡터를 만들어서 진행해 보는 것?\n",
    "- N-gram으로 추가해서 성능을 향상시킬 수 있지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8f3468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/jun/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/jun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# NLTK의 movie_reviews 데이터셋 다운로드\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 영화 리뷰와 해당 라벨을 데이터셋으로 로드\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# 데이터를 셔플링하여 무작위로 섞음\n",
    "random.shuffle(documents)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7aeecefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.문장 및 단어 정규화\n",
    "def normalize_text(text):\n",
    "    text = text.lower() #소문자 \n",
    "    text = re.sub(r'\\d+','',text) # 숫자 제거\n",
    "    text = re.sub(r'[^\\w\\s]','',text) #구두점 제거\n",
    "    return text\n",
    "\n",
    "# 전처리 진행\n",
    "normalized_reviews =[normalize_text(' '.join(review)) for review, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e670721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.어간추출, 표제어 추출\n",
    "# 어간추출 (stemming)\n",
    "stemmer=PorterStemmer()\n",
    "stemmed_reviews =[' '.join([stemmer.stem(word) for word in review.split()]) for review in normalized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7bb99a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 표제어 추출 \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatized_reviews=[' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in normalized_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2637117",
   "metadata": {},
   "source": [
    "--- \n",
    "- 정규식, nltk 패키지로 리뷰 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac8fca",
   "metadata": {},
   "source": [
    "---\n",
    "- 임베딩을 통한 행렬 변환 \n",
    "\n",
    "--- \n",
    "- n_gram으로 추가하여서 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fc9ff",
   "metadata": {},
   "source": [
    "### 어간추출로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c5bbe175",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f47a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. countvectorizer 하이퍼파라미터 조정\n",
    "# 최소빈도수 설정해서 희귀 단어 제거 가능\n",
    "\n",
    "## 2번의 어간추출, 표제어 추출로 학습 테스트\n",
    "vectorizer_min_df = CountVectorizer(min_df=2)\n",
    "X_min_df =vectorizer_min_df.fit_transform(lemmatized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97e953e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-그램 사용\n",
    "# 2,2\n",
    "vectorizer_bigram=CountVectorizer(ngram_range=(2,2))\n",
    "X_bigram=vectorizer_bigram.fit_transform(lemmatized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7bd23a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer=TfidfVectorizer()\n",
    "X_tfidf=tfidf_vectorizer.fit_transform(lemmatized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5878130",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터를 분할해야 한다.\n",
    "\n",
    "#1. stem\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem=train_test_split(stemmed_reviews, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "#2. X_min_df\n",
    "X_train_min_df, X_test_min_df, y_train_min_df, y_test_min_df = train_test_split(X_min_df, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "#3. X_bigram\n",
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_bigram, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "#4. TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "\n",
    "## Naive Bayes 분류기 사용하여 학습, 평가\n",
    "\n",
    "#1. Navie Bayes 분류기 학습\n",
    "model_stem =MultinomialNB()\n",
    "model_stem.fit(vectorizer.fit_transform(X_train_stem), y_train_stem)\n",
    "y_pred_stem= model_stem.predict(vectorizer.transform(X_test_stem))\n",
    "accuracy_stem = accuracy_score(y_test_stem, y_pred_stem)\n",
    "\n",
    "#2. min_df Navie Bayes 분류기 학습\n",
    "model_min_df =MultinomialNB()\n",
    "model_min_df.fit(X_train_min_df, y_train_min_df)\n",
    "y_pred_min_df= model_min_df.predict(X_test_min_df)\n",
    "accuracy_min_df = accuracy_score(y_test_min_df, y_pred_min_df)\n",
    "\n",
    "#3. X_bigram Navie Bayes 분류기 학습\n",
    "model_bigram =MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, y_train_bigram)\n",
    "y_pred_bigram= model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test_bigram, y_pred_bigram)\n",
    "\n",
    "#4. TF-IDF Navie Bayes 분류기 학습\n",
    "model_tfidf =MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf= model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "abb38aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stem & Countvectorizer': 0.82,\n",
       " 'stem & min_df': 0.8275,\n",
       " 'stem & bigram': 0.85,\n",
       " 'stem & tfidf': 0.8025}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    'stem & Countvectorizer' : accuracy_stem,\n",
    "    'stem & min_df' :accuracy_min_df,\n",
    "    'stem & bigram': accuracy_bigram,\n",
    "    'stem & tfidf' : accuracy_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f6caa",
   "metadata": {},
   "source": [
    "### 표제어 추출로 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d921da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 4. countvectorizer 하이퍼파라미터 조정\n",
    "# 최소빈도수 설정해서 희귀 단어 제거 가능\n",
    "\n",
    "## 2번의 어간추출, 표제어 추출로 학습 테스트\n",
    "vectorizer_min_df = CountVectorizer(min_df=2)\n",
    "X_min_df =vectorizer_min_df.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# N-그램 사용\n",
    "# 2,2\n",
    "vectorizer_bigram=CountVectorizer(ngram_range=(2,2))\n",
    "X_bigram=vectorizer_bigram.fit_transform(lemmatized_reviews)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer=TfidfVectorizer()\n",
    "X_tfidf=tfidf_vectorizer.fit_transform(lemmatized_reviews)\n",
    "\n",
    "\n",
    "## 데이터를 분할해야 한다.\n",
    "\n",
    "#1. stem\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem=train_test_split(lemmatized_reviews, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "#2. X_min_df\n",
    "X_train_min_df, X_test_min_df, y_train_min_df, y_test_min_df = train_test_split(X_min_df, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "#3. X_bigram\n",
    "X_train_bigram, X_test_bigram, y_train_bigram, y_test_bigram = train_test_split(X_bigram, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "#4. TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, [label for _, label in documents], test_size=0.2, random_state=111)\n",
    "\n",
    "\n",
    "## Naive Bayes 분류기 사용하여 학습, 평가\n",
    "\n",
    "#1. Navie Bayes 분류기 학습\n",
    "model_stem =MultinomialNB()\n",
    "model_stem.fit(vectorizer.fit_transform(X_train_stem), y_train_stem)\n",
    "y_pred_stem= model_stem.predict(vectorizer.transform(X_test_stem))\n",
    "accuracy_stem = accuracy_score(y_test_stem, y_pred_stem)\n",
    "\n",
    "#2. min_df Navie Bayes 분류기 학습\n",
    "model_min_df =MultinomialNB()\n",
    "model_min_df.fit(X_train_min_df, y_train_min_df)\n",
    "y_pred_min_df= model_min_df.predict(X_test_min_df)\n",
    "accuracy_min_df = accuracy_score(y_test_min_df, y_pred_min_df)\n",
    "\n",
    "#3. X_bigram Navie Bayes 분류기 학습\n",
    "model_bigram =MultinomialNB()\n",
    "model_bigram.fit(X_train_bigram, y_train_bigram)\n",
    "y_pred_bigram= model_bigram.predict(X_test_bigram)\n",
    "accuracy_bigram = accuracy_score(y_test_bigram, y_pred_bigram)\n",
    "\n",
    "#4. TF-IDF Navie Bayes 분류기 학습\n",
    "model_tfidf =MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_tfidf= model_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test_tfidf, y_pred_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b619cd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma & Countvectorizer': 0.8275,\n",
       " 'lemma & min_df': 0.8275,\n",
       " 'lemma & bigram': 0.85,\n",
       " 'lemma & tfidf': 0.8025}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    'lemma & Countvectorizer' : accuracy_stem,\n",
    "    'lemma & min_df' :accuracy_min_df,\n",
    "    'lemma & bigram': accuracy_bigram,\n",
    "    'lemma & tfidf' : accuracy_tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec1b3309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600x474149 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 912105 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd8d0d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600x21210 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 503872 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d98bc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600x34758 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 514716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dde08a",
   "metadata": {},
   "source": [
    "- I Love BDA \n",
    "- 2-그램\n",
    "- 'I Love', 'Love BDA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9a403",
   "metadata": {},
   "source": [
    "### 필수과제1\n",
    "- base로만 진행했지만, 추가적으로 임베딩을 다양하게 진행해 보시면서 0.845 의 성능보다 더 올리기\n",
    "- 다양한 텍스트 전처리를 통해 성능 개선하기 \n",
    "- 파생변수를 추가해도 괜찮음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c9d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
